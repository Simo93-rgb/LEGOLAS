#!/bin/bash
# Script helper per training e evaluation con storie XES
#
# FUNZIONALITÃ€:
# - Scelta formato storie (narrativo/bullet/clinical)
# - Scelta modello (da YAML o legacy)
# - K-Fold Cross Validation (opzionale)
# - Focal Loss (opzionale, per classi sbilanciate)
# - Configurazione hyperparameters (epochs, patience, batch_size, lr)
# - Training + Evaluation automatici
#
# AGGIORNATO: 9 Ottobre 2025 - Supporto K-Fold e Focal Loss (FASE 4)
#
# USO: ./scripts/launch_run_train_eval
#      Menu interattivo guida attraverso tutte le opzioni
#
# DOCUMENTAZIONE: docs/TRAIN_LLM_INTEGRATION.md

# ============================================================
# PATH CONFIGURATION
# ============================================================
# NOTA: I path sono gestiti centralmente in src/config/paths.py
# Questi path devono corrispondere a quelli definiti in paths.py
STORIES_DIR="output/stories"
MODELS_DIR="output/models"
EVALUATION_DIR="output/evaluation"
PREDICTION_DIR="prediction"

# Colori
GREEN='\033[0;32m'
BLUE='\033[0;34m'
YELLOW='\033[1;33m'
RED='\033[0;31m'
NC='\033[0m' # No Color

echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${BLUE}   LEGOLAS - Training & Evaluation Pipeline${NC}"
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"

# Verifica che i file di storie esistano
if [ ! -d "${STORIES_DIR}" ] || [ ! -f "${STORIES_DIR}/narrativo_train.pkl" ]; then
    echo -e "${RED}âŒ Errore: File di storie non trovati!${NC}"
    echo "   Esegui prima: ./scripts/run_xes_pipeline.sh"
    exit 1
fi

echo -e "${GREEN}âœ… File di storie trovati in ${STORIES_DIR}/${NC}\n"

# Assicura che la directory evaluation esista
mkdir -p "${EVALUATION_DIR}"

# Menu scelta formato
echo -e "${YELLOW}Scegli il formato delle storie per il training:${NC}"
echo "  1) narrativo (raccomandato per BERT)"
echo "  2) bullet (formato compatto)"
echo "  3) clinical (con token clinici - sperimentale)"
echo ""
read -p "Scelta [1-3, default=1]: " choice

case $choice in
    2) FORMAT="bullet" ;;
    3) FORMAT="clinical" ;;
    *) FORMAT="narrativo" ;;
esac

echo -e "\n${GREEN}ðŸ“ Formato selezionato: ${FORMAT}${NC}\n"

# Menu scelta modello
echo -e "\n${YELLOW}Scegli il modello da usare:${NC}"
echo ""

# Usa script Python per caricare modelli da YAML
SCRIPT_DIR="$(cd "$(dirname "${BASH_SOURCE[0]}")" && pwd)"
MODEL_MENU=$("$SCRIPT_DIR/list_models.py" menu 2>/dev/null)

if [ $? -eq 0 ] && [ -n "$MODEL_MENU" ]; then
    # Menu dinamico da YAML
    echo -e "${GREEN}ðŸ“‹ Modelli disponibili da config/model_configs.yaml:${NC}"
    echo ""
    
    # Parse e display menu
    declare -a MODEL_IDS
    declare -a MODEL_DESCS
    i=0
    
    while IFS='|' read -r num model_id desc; do
        MODEL_IDS[$i]="$model_id"
        MODEL_DESCS[$i]="$desc"
        echo "  $num) $model_id"
        echo "      â””â”€ $desc"
        i=$((i+1))
    done <<< "$MODEL_MENU"
    
    MODEL_COUNT=$i
    echo ""
    read -p "Scelta [1-${MODEL_COUNT}, default=1]: " model_choice
    
    # Valida scelta
    if [[ "$model_choice" =~ ^[0-9]+$ ]] && [ "$model_choice" -ge 1 ] && [ "$model_choice" -le "$MODEL_COUNT" ]; then
        idx=$((model_choice - 1))
        MODEL="${MODEL_IDS[$idx]}"
    else
        # Default al primo
        MODEL="${MODEL_IDS[0]}"
    fi
    
    echo -e "\n${GREEN}ðŸ¤– Modello selezionato: ${MODEL}${NC}"
    
    # Mostra info modello
    MODEL_INFO=$("$SCRIPT_DIR/list_models.py" info "$MODEL" 2>/dev/null)
    if [ $? -eq 0 ]; then
        eval "$MODEL_INFO"
        echo -e "   HuggingFace: ${BLUE}${HF_ID}${NC}"
        echo -e "   Tipo: ${TYPE}"
        echo -e "   Batch consigliato: ${BATCH}"
        echo -e "   Learning rate: ${LR}"
    fi
    
else
    # Fallback a menu legacy
    echo -e "${YELLOW}âš ï¸  Config YAML non trovato, uso modelli legacy${NC}"
    echo ""
    echo "  1) bertm (BERT Medium - default)"
    echo "  2) roberta (RoBERTa Base)"
    echo "  3) cbert (Clinical BERT)"
    echo "  4) gpt2 (GPT-2)"
    echo ""
    read -p "Scelta [1-4, default=1]: " model_choice

    case $model_choice in
        2) MODEL="roberta" ;;
        3) MODEL="cbert" ;;
        4) MODEL="gpt2" ;;
        *) MODEL="bertm" ;;
    esac
    
    echo -e "\n${GREEN}ðŸ¤– Modello selezionato: ${MODEL}${NC}\n"
fi

# ============================================================
# MENU OPZIONI TRAINING AVANZATE
# ============================================================

# Menu K-Fold
echo -e "${YELLOW}Vuoi usare K-Fold Cross Validation?${NC}"
echo "  1) No - Training semplice (piÃ¹ veloce, per test)"
echo "  2) SÃ¬ - K-Fold CV (piÃ¹ robusto, metriche affidabili)"
echo ""
read -p "Scelta [1-2, default=1]: " kfold_choice

USE_KFOLD=""
N_FOLDS=10

case $kfold_choice in
    2)
        USE_KFOLD="--use_kfold"
        echo ""
        read -p "Numero di folds [default=10]: " n_folds_input
        if [[ "$n_folds_input" =~ ^[0-9]+$ ]] && [ "$n_folds_input" -ge 2 ]; then
            N_FOLDS=$n_folds_input
        fi
        echo -e "\n${GREEN}ðŸ”„ K-Fold attivato: ${N_FOLDS} folds${NC}"
        ;;
    *)
        echo -e "\n${GREEN}âš¡ Training semplice (no K-Fold)${NC}"
        ;;
esac

# Menu Loss Function
echo ""
echo -e "${YELLOW}Quale Loss Function vuoi usare?${NC}"
echo "  1) Cross Entropy (default, con class weights automatici)"
echo "  2) Focal Loss (per classi molto sbilanciate)"
echo ""
read -p "Scelta [1-2, default=1]: " loss_choice

USE_FOCAL=""
FOCAL_ALPHA="0.25 0.75"
FOCAL_GAMMA="2.0"

case $loss_choice in
    2)
        USE_FOCAL="--use_focal_loss"
        echo ""
        echo -e "${YELLOW}Configurazione Focal Loss:${NC}"
        read -p "Alpha per classe 0 [default=0.25]: " alpha0
        read -p "Alpha per classe 1 [default=0.75]: " alpha1
        read -p "Gamma (focusing) [default=2.0]: " gamma
        
        # Valida input
        if [[ "$alpha0" =~ ^[0-9.]+$ ]]; then
            FOCAL_ALPHA="$alpha0 ${alpha1:-0.75}"
        fi
        if [[ "$gamma" =~ ^[0-9.]+$ ]]; then
            FOCAL_GAMMA="$gamma"
        fi
        
        echo -e "\n${GREEN}ðŸŽ¯ Focal Loss: Î±=[${FOCAL_ALPHA}], Î³=${FOCAL_GAMMA}${NC}"
        ;;
    *)
        echo -e "\n${GREEN}ðŸ“Š Cross Entropy con class weights automatici${NC}"
        ;;
esac

# Menu Hyperparameters
echo ""
echo -e "${YELLOW}Configurazione Training:${NC}"
read -p "Numero epoche [default=10]: " epochs
read -p "Patience early stopping [default=5]: " patience
read -p "Batch size [default=256]: " batch_size
read -p "Learning rate [default=1e-5]: " lr

# Defaults
EPOCHS=${epochs:-10}
PATIENCE=${patience:-5}
BATCH_SIZE=${batch_size:-256}
LR=${lr:-1e-5}

echo ""
echo -e "${GREEN}âœ… Configurazione:${NC}"
echo "   Epochs: ${EPOCHS}"
echo "   Patience: ${PATIENCE}"
echo "   Batch size: ${BATCH_SIZE}"
echo "   Learning rate: ${LR}"

# Costruisci comando training
TRAIN_CMD="uv run python src/training/train_llm.py \
    --story_format ${FORMAT} \
    --model ${MODEL} \
    --epochs ${EPOCHS} \
    --patience ${PATIENCE} \
    --batch_size ${BATCH_SIZE} \
    --learning_rate ${LR}"

# Aggiungi opzioni K-Fold
if [ -n "$USE_KFOLD" ]; then
    TRAIN_CMD="${TRAIN_CMD} ${USE_KFOLD} --n_folds ${N_FOLDS}"
fi

# Aggiungi opzioni Focal Loss
if [ -n "$USE_FOCAL" ]; then
    TRAIN_CMD="${TRAIN_CMD} ${USE_FOCAL} --focal_alpha ${FOCAL_ALPHA} --focal_gamma ${FOCAL_GAMMA}"
fi

# ============================================================
# MENU AZIONE
# ============================================================

echo ""
echo -e "${YELLOW}Cosa vuoi fare?${NC}"
echo "  1) Solo training"
echo "  2) Solo evaluation (modello giÃ  addestrato)"
echo "  3) Training + Evaluation (completo)"
echo ""
read -p "Scelta [1-3, default=3]: " action

echo ""

case $action in
    1)
        echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
        echo -e "${BLUE}   TRAINING${NC}"
        echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"
        echo -e "${YELLOW}Comando: ${TRAIN_CMD}${NC}\n"
        eval ${TRAIN_CMD}
        ;;
    2)
        echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
        echo -e "${BLUE}   EVALUATION${NC}"
        echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"
        
        # Genera timestamp per file unico
        TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
        EVAL_LOG="${EVALUATION_DIR}/eval_${FORMAT}_${MODEL}_${TIMESTAMP}.log"
        
        echo -e "${BLUE}ðŸ“ Output salvato in: ${EVAL_LOG}${NC}\n"
        
        # Aggiorna anche eval_model.py con il modello scelto
        sed -i "s/model_name = '.*'/model_name = '${MODEL}'/" src/training/eval_model.py
        
        # Esegui e salva output su file (con tee per mostrare anche a schermo)
        uv run python src/training/eval_model.py 2>&1 | tee "${EVAL_LOG}"
        
        EVAL_EXIT_CODE=${PIPESTATUS[0]}
        
        if [ $EVAL_EXIT_CODE -eq 0 ]; then
            echo -e "\n${GREEN}âœ… Log salvato in: ${EVAL_LOG}${NC}"
        else
            echo -e "\n${RED}âŒ Errore durante evaluation (log: ${EVAL_LOG})${NC}"
        fi
        ;;
    *)
        echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
        echo -e "${BLUE}   TRAINING${NC}"
        echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"
        echo -e "${YELLOW}Comando: ${TRAIN_CMD}${NC}\n"
        eval ${TRAIN_CMD}
        
        if [ $? -eq 0 ]; then
            echo -e "\n${GREEN}âœ… Training completato!${NC}\n"
            echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
            echo -e "${BLUE}   EVALUATION${NC}"
            echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"
            
            # Genera timestamp per file unico
            TIMESTAMP=$(date +"%Y%m%d_%H%M%S")
            EVAL_LOG="${EVALUATION_DIR}/eval_${FORMAT}_${MODEL}_${TIMESTAMP}.log"
            
            echo -e "${BLUE}ðŸ“ Output salvato in: ${EVAL_LOG}${NC}\n"
            
            # Aggiorna anche eval_model.py con il modello scelto
            sed -i "s/model_name = '.*'/model_name = '${MODEL}'/" src/training/eval_model.py
            
            # Esegui e salva output su file (con tee per mostrare anche a schermo)
            uv run python src/training/eval_model.py 2>&1 | tee "${EVAL_LOG}"
            
            EVAL_EXIT_CODE=${PIPESTATUS[0]}
            
            if [ $EVAL_EXIT_CODE -eq 0 ]; then
                echo -e "\n${GREEN}âœ… Evaluation completato!${NC}"
                echo -e "ðŸ“„ Log salvato in: ${GREEN}${EVAL_LOG}${NC}"
                echo -e "ðŸ“Š Risultati salvati in: ${GREEN}${PREDICTION_DIR}/xes_${FORMAT}_${MODEL}_*${NC}"
            else
                echo -e "\n${RED}âŒ Errore durante evaluation (log: ${EVAL_LOG})${NC}"
            fi
        else
            echo -e "\n${RED}âŒ Errore durante il training${NC}"
            exit 1
        fi
        ;;
esac

echo -e "\n${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}"
echo -e "${GREEN}âœ… Operazione completata!${NC}"
echo -e "${BLUE}â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•${NC}\n"

# Mostra file generati
if [ -d "${PREDICTION_DIR}" ]; then
    echo "ðŸ“ File generati:"
    ls -lh ${PREDICTION_DIR}/xes_${FORMAT}_${MODEL}_* 2>/dev/null || echo "   (nessun file ancora)"
    echo ""
fi

echo -e "${YELLOW}ðŸ’¡ Suggerimenti:${NC}"

# Mostra path diversi se K-Fold o simple
if [ -n "$USE_KFOLD" ]; then
    echo "   - Modelli salvati (${N_FOLDS} fold): ${MODELS_DIR}/best_model_${FORMAT}_${MODEL}_fold*.pth"
    echo "   - Risultati aggregati: ${MODELS_DIR}/../reports/kfold_aggregated_results.json"
    echo "   - Metriche per fold: ${MODELS_DIR}/../reports/fold_*_metrics.json"
else
    echo "   - Modello salvato in: ${MODELS_DIR}/best_model_${FORMAT}_${MODEL}.pth"
    echo "   - Checkpoint history: ${MODELS_DIR}/../reports/checkpoint_history.json"
fi

echo "   - Label mapping: ${MODELS_DIR}/../reports/label_mapping.json"
echo "   - Report evaluation: ${PREDICTION_DIR}/xes_${FORMAT}_${MODEL}_report.txt"
echo "   - Log evaluation: ${EVALUATION_DIR}/eval_${FORMAT}_${MODEL}_*.log"
echo ""
echo -e "${YELLOW}ðŸ“š Documentazione:${NC}"
echo "   - Guida completa: docs/TRAIN_LLM_INTEGRATION.md"
echo "   - Path management: src/config/paths.py"
echo ""
echo -e "${YELLOW}ðŸ”„ Per rilanciare:${NC}"
echo "   - Stesso script: ./scripts/launch_run_train_eval"
echo "   - CLI diretta: uv run python src/training/train_llm.py --help"
